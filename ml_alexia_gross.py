# -*- coding: utf-8 -*-
"""ML_Alexia_Gross.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QmIKGSKQE_9_X4AObFPXOpoHdChmelBR

# **Machine Learning Project : Movie popularity prediction**

**Data Source**

* https://www.kaggle.com/hoangdang89/ticket-box-prediction
* https://github.com/busyML/Box-Office-Predictions/blob/master/Predicting_Film_Revenue_A_Soft_Intro_to_Neural_Networks.ipynb

**Library Loading**
"""

import numpy as np
import os, sys, sklearn
import matplotlib.pyplot as plt
import seaborn as sns


import pandas as pd
pd.options.display.float_format = '{:,.4f}'.format

import warnings
warnings.filterwarnings(action="ignore", message="^internal gelsd")

import csv

"""**Data Loading**"""

#train_set = pd.read_csv('/content/train.csv', header = None, delimiter="\t", quoting=csv.QUOTE_NONE, encoding='utf-8')

train_set = pd.read_csv('/content/train.csv')
test_set = pd.read_csv('/content/test.csv')

train_set.head()

test_set.info()

train_set.info()

"""## **Part 1 : Data Visualization**

___
Parameters
____
"""

label_column = "budget"

info = train_set[label_column].copy()
info.describe()

label_column = "popularity"

info = train_set[label_column].copy()
info.describe()

label_column = "runtime"

info = train_set[label_column].copy()
info.describe()

label_column = "revenue"

info = train_set[label_column].copy()
info.describe()

"""___
Histograms
___
"""

# plot histogram for all columns
train_set.hist(bins=50, figsize=(12,9)) # bins is number of groups of values
plt.show()

# histogram and kernel density estimation function of the variable popularity

label_column = "popularity"
info = train_set[label_column].copy()

ax = sns.distplot(info, hist=True, hist_kws={"edgecolor": 'w', "linewidth": 3}, kde_kws={"linewidth": 3})

# notation indicating a possible outlier
ax.annotate('Possible outlier', xy=(188,0.0030), xytext=(189,0.0070), fontsize=12,
            arrowprops=dict(arrowstyle='->', ec='grey', lw=2), bbox = dict(boxstyle="round", fc="0.8"))

# ticks 
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)

# labels and title
plt.xlabel('Popularity', fontsize=14)
plt.ylabel('frequency', fontsize=14)
plt.title('Distribution of popularity', fontsize=20);

sns.set(rc={'figure.figsize':(10,10)})

label_column = "runtime"
info = train_set[label_column].copy()

ax = sns.distplot(info, hist=True, hist_kws={"edgecolor": 'w', "linewidth": 3}, kde_kws={"linewidth": 3})

# notation indicating a possible outlier
ax.annotate('Possible outlier', xy=(250,0.0010), xytext=(189,0.0070), fontsize=12,
            arrowprops=dict(arrowstyle='->', ec='grey', lw=2), bbox = dict(boxstyle="round", fc="0.8"))

# ticks 
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)

# labels and title
plt.xlabel('Runtime', fontsize=14)
plt.ylabel('frequency', fontsize=14)
plt.title('Distribution of runtime', fontsize=20);

sns.set(rc={'figure.figsize':(10,10)})

"""___
Box plots
___
"""

# box plot of the variable popularity

label_column = "popularity"
info = train_set[label_column].copy()

ax = sns.boxplot(info)

# notation indicating an outlier
ax.annotate('Outlier', xy=(190,0), xytext=(186,-0.05), fontsize=14,
            arrowprops=dict(arrowstyle='->', ec='grey', lw=2), bbox = dict(boxstyle="round", fc="0.8"))

# xtick, label, and title
plt.xticks(fontsize=14)
plt.xlabel('popularity', fontsize=14)
plt.title('Distribution of popularity', fontsize=20)

sns.set(rc={'figure.figsize':(20,20)})

# box plot of the variable budget

label_column = "budget"
info = train_set[label_column].copy()

ax = sns.boxplot(info)

# notation indicating an outlier
ax.annotate('Outlier', xy=(300000000,0), xytext=(300000000,-0.05), fontsize=14,
            arrowprops=dict(arrowstyle='->', ec='grey', lw=2), bbox = dict(boxstyle="round", fc="0.8"))

# xtick, label, and title
plt.xticks(fontsize=14)
plt.xlabel('budget', fontsize=14)
plt.title('Distribution of budget', fontsize=20)

sns.set(rc={'figure.figsize':(10,10)})

# box plot of the variable runtime

label_column = "runtime"
info = train_set[label_column].copy()

ax = sns.boxplot(info)

# notation indicating an outlier
ax.annotate('Outlier', xy=(190,0), xytext=(186,-0.05), fontsize=14,
            arrowprops=dict(arrowstyle='->', ec='grey', lw=2), bbox = dict(boxstyle="round", fc="0.8"))

# xtick, label, and title
plt.xticks(fontsize=14)
plt.xlabel('runtime', fontsize=14)
plt.title('Distribution of runtime', fontsize=20)

sns.set(rc={'figure.figsize':(10,10)})

# box plot of the variable revenue

label_column = "revenue"
info = train_set[label_column].copy()

ax = sns.boxplot(info)

# notation indicating an outlier
ax.annotate('Outlier', xy=(1e9,0), xytext=(1e9,-0.05), fontsize=14,
            arrowprops=dict(arrowstyle='->', ec='grey', lw=2), bbox = dict(boxstyle="round", fc="0.8"))

# xtick, label, and title
plt.xticks(fontsize=14)
plt.xlabel('revenue', fontsize=14)
plt.title('Distribution of revenue', fontsize=20)

sns.set(rc={'figure.figsize':(10,10)})

df = train_set
Q1 = train_set.quantile(0.25)
Q3 = train_set.quantile(0.75)
IQR = Q3 - Q1

((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).sum()

"""___
Correlation
___
"""

corr = train_set.corr()
ax = sns.heatmap(
    corr, 
    vmin=-1, vmax=1, center=0,
    cmap=sns.diverging_palette(230, 20, as_cmap=True),
    square=True,
    annot=True,
    linewidths=.5
)
ax.set_xticklabels(
    ax.get_xticklabels(),
    rotation=45,
    horizontalalignment='right'
);

"""We can observe that the "Budget" is highly corrolated with the "revenue". On the contrary, the "Popularity" is not corrolated with the "Runtime".

___
Analysis
___
"""

most_popular_movies = train_set.sort_values('popularity', ascending=False).head(n=20)
most_popular_movies['revenue(million)'] = most_popular_movies['revenue'].apply(lambda x : x//1000000)
most_popular_movies['budget(million)'] = most_popular_movies['budget'].apply(lambda x : x//1000000)

plt.figure(figsize=(12, 10))
ax = sns.barplot(y='original_title', x='popularity', data=most_popular_movies, order=most_popular_movies.sort_values('popularity', ascending=False).original_title, orient='h')
for p in ax.patches:
        ax.annotate('{}'.format(int(p.get_width())), (p.get_width(), p.get_y()+0.5), fontsize=12)
plt.title('Top 20 Most Popular Movies', fontsize=12)
plt.ylabel('')
plt.show()

label_column = "popularity"

info = train_set[label_column].copy()
info.describe()

train_set['popularity'] = train_set['popularity'].mask(train_set['popularity'] >= 40, 40)

most_popular_movies = train_set.sort_values('popularity', ascending=False).head(n=20)
most_popular_movies['revenue(million)'] = most_popular_movies['revenue'].apply(lambda x : x//1000000)
most_popular_movies['budget(million)'] = most_popular_movies['budget'].apply(lambda x : x//1000000)

plt.figure(figsize=(12, 10))
ax = sns.barplot(y='original_title', x='popularity', data=most_popular_movies, order=most_popular_movies.sort_values('popularity', ascending=False).original_title, orient='h')
for p in ax.patches:
        ax.annotate('{}'.format(int(p.get_width())), (p.get_width(), p.get_y()+0.5), fontsize=12)
plt.title('Top 20 Most Popular Movies', fontsize=12)
plt.ylabel('')
plt.show()

label_column = "popularity"

info = train_set[label_column].copy()
info.describe()

"""___
**Dataset Creation**
___
"""

SPLITTER = " "
COLUMN_EXCLUDE_PATTERN = "id|popularity|belongs_to_collection|homepage|tagline|revenue"

TEXT_FIELDS =[  ("genres", "name"), 
                ("production_companies", "id"),                
                ("production_countries", "iso_3166_1"),
                ("spoken_languages", "name"),
                ("Keywords", "name"),
                ("cast", "name"),                         
                ("crew", "name"),              
             ]

TEXT_FIELDS2 =[ ("production_companies", "name"),
                ("cast", "character"),  
                ("cast", "job"),  
                ("cast", "profile_path"),
                ("crew", "job"),
                ("crew", "department"),
             ]

label_column = "popularity"
X_train = train_set.copy().drop(train_set.filter(regex=COLUMN_EXCLUDE_PATTERN), axis=1)
y_train = train_set[label_column].copy()

X_train.shape

"""## **Part 2 : Pre-treatment of the data**

Visualization of the following attributes : budget, original title, popularity and original language
"""

from sklearn.base import BaseEstimator, TransformerMixin

class LimitedColumnsFilter(BaseEstimator, TransformerMixin):
    def __init__(self, filters):
        self.filters = filters   

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):        
        return X.copy().filter(items=self.filters)

filters = ('budget', 'original_title','revenue', 'original_language')
result = LimitedColumnsFilter(filters).transform(X_train)
result.head()

result

"""We convert the dates from "2/20/15" to "2015-02-20" in order to be used in our model."""

class DateTimeImputer(BaseEstimator, TransformerMixin):
    def __init__(self, replace=True):
        self.replace = replace
        pass

    def fit(self, X, y=None):
        return self
        
    def transform(self, X, y=None):
        clone_X = X.copy()                            
        for feature in X.select_dtypes(include=[np.object]).columns:
            try:
                clone_X[feature] = pd.to_datetime(X[feature], infer_datetime_format=True)
            except:
                pass
        return clone_X

result = DateTimeImputer().transform(X_train)
filters = list(X_train.filter(like="date").columns)
result[filters].head()

"""We transform the date from "2015-02-20" to three variables : y=2015, m=02, d=20"""

class DateDissolver(BaseEstimator, TransformerMixin):
    def __init__(self, replace=False): # no *args or **kargs
        self.replace = replace
        pass

    def fit(self, X, y=None):
        return self  

    def transform(self, X, y=None):
        clone_X = X.copy()
        for feature in X.select_dtypes(include=[np.datetime64]).columns:
            if self.replace:
                clone_X = clone_X.drop([feature], axis=1)                  
            try:        
                clone_X['{0}_Y'.format(feature)] = X[feature].dt.year
                clone_X['{0}_M'.format(feature)] = X[feature].dt.month
                clone_X['{0}_D'.format(feature)] = X[feature].dt.day
            except:
                pass
        return clone_X

result = DateTimeImputer().transform(X_train)
result = DateDissolver(replace=True).transform(result)
filters = list(result.filter(like="date").columns)
result[filters].head()

"""Visualization of the following variables: budget, popularity and duration."""

class NumberFilter(BaseEstimator, TransformerMixin):
    def __init__(self): # no *args or **kargs
        pass
    def fit(self, X, y=None):
        return self  # nothing else to do
    def transform(self, X, y=None):      
        return X.copy().select_dtypes(include=[np.int64, np.float64])        

result = NumberFilter().transform(X_train)
result.head()

"""List of variables : result.columns"""

class CategoryFilter(BaseEstimator, TransformerMixin):
    def __init__(self):
        pass

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):        
        return X.copy().select_dtypes(include=[np.object])        

result = CategoryFilter().transform(X_train)
result.columns

"""Traitment of the other variables: belongs_to_collection, genres, production_companies, production_countries, spoken_languages, cast, crew, keywords"""

import ast

class InfoExtractor(BaseEstimator, TransformerMixin):
    def __init__(self, field, replace=False):
        self.field = field
        self.replace = replace

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        clone_X = X.copy()   
        for feature, field_name in self.field:
            if self.replace:
                clone_X[feature] = X[feature].apply(lambda x: self.extract_field(x, field_name))
            else:
                clone_X["{0}_{1}".format(feature, field_name)] = X[feature].apply(lambda x: self.extract_field(x, field_name))
        return clone_X
        
    def extract_field(self, data, field_name):
        if(data is not np.nan):
            info = ast.literal_eval(data)            
            result = SPLITTER.join("{}".format(x[field_name]).replace(SPLITTER, "_") for x in info)
            return result
        return np.nan
    
infoExtractor = InfoExtractor(field=TEXT_FIELDS, replace=True)
result = infoExtractor.transform(X_train)
filters = list(result.filter(regex="cast|production|genres|languages|Keywords|crew").columns)
result[filters].head()

"""Changement of the "Null" variables."""

from keras.preprocessing.text import Tokenizer

class TextEncoder(BaseEstimator, TransformerMixin):
    def __init__(self, field=None, replace=False):
        self.field = field
        self.replace = replace

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        clone_X = X.copy()
        if self.field is None:
            self.field = X.copy().select_dtypes(include=[np.object], exclude=[np.datetime64]).columns               
        for feature in self.field:               
            if self.replace:               
                clone_X[feature] = pd.Series(data=self.encode_textBySum(X[feature]), index=clone_X.index)
            else:                
                clone_X["{0}_{1}".format(feature, 'count')] = pd.Series(data=self.encode_textBySum(X[feature]), index=clone_X.index)
        return clone_X
        
    def encode_textBySum(self, df_feature):
        tokenizer = Tokenizer()
        clone_feature = df_feature.copy().fillna('')        
        tokenizer.fit_on_texts(clone_feature)
       
        encoded_docs = tokenizer.texts_to_matrix(clone_feature, mode='tfidf')
        encoded_nums = np.sum(encoded_docs,axis=1)
        return encoded_nums
    
    def encode_textForOneHot(self, df_feature):
        tokenizer = Tokenizer()
        clone_feature = df_feature.copy().fillna('')        
        tokenizer.fit_on_texts(clone_feature)
        encoded_docs = tokenizer.texts_to_matrix(clone_feature, mode='binary')        
        encoded_onehot = pd.DataFrame(data=encoded_docs).applymap("{:1.0f}".format).apply("".join, axis=1)                      
        return encoded_onehot


infoExtractor = InfoExtractor(field=[("cast", "name")], replace=True)                
textEncoder = TextEncoder(field=["cast"], replace=True)
result = infoExtractor.transform(X_train)
result = textEncoder.transform(result)
filters = list(result.filter(regex="date|cast").columns)
result[filters].head()

"""___
**Pipelines Transformation**
___

Data used for our model.
"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.impute import SimpleImputer


text_pipeline = Pipeline([        
        ('inf_extor'   , InfoExtractor(field=TEXT_FIELDS, replace=True)),
        ('txt_encoder' , TextEncoder(replace=True)),
    ])

result = DateTimeImputer().transform(X_train)
result = DateDissolver(replace=True).transform(result)
result = text_pipeline.fit_transform(result)
result.head()

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.impute import SimpleImputer

num_pipeline = Pipeline([        
        ('num_filter', NumberFilter()),
        ('imputer'   , SimpleImputer(strategy="median")),     # fill nan/empty cells        
        ('mm_scaler' , MinMaxScaler(feature_range=(-1, 1))),  # feature scaling
    ])

result = num_pipeline.fit_transform(X_train)
pd.DataFrame(data=result).describe()

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer

cat_pipeline = Pipeline([
        ('cat_filter', CategoryFilter()),
        ('imputer'   , SimpleImputer(strategy='constant', fill_value='Missing')),  # fill nan/empty cells
    ])

filters = ('budget', 'original_language')
result = LimitedColumnsFilter(filters).transform(X_train)
result = cat_pipeline.fit_transform(result)
result[0]

from sklearn.compose import ColumnTransformer
from sklearn.compose import make_column_transformer
from sklearn.compose import make_column_selector


full_pipeline = make_column_transformer(              
    (num_pipeline , make_column_selector(dtype_include=[np.int64, np.float64])),            
)


filters = list(X_train.filter(regex="date|budget|original_language").columns)
result = LimitedColumnsFilter(filters).transform(X_train)
result = full_pipeline.fit_transform(result)

"""## **Part 3 : Model creation**

### **First model**
"""

import tensorflow as tf
from tensorflow import keras
from keras.layers import Dense
from tensorflow.keras import Model
from keras.layers import Dropout

X_train_pp_df = DateTimeImputer().transform(X_train)
X_train_pp_df = DateDissolver(replace=True).transform(X_train_pp_df)
X_train_pp_df = text_pipeline.fit_transform(X_train_pp_df)

X_train_pp = full_pipeline.fit_transform(X_train_pp_df)

model_1 = tf.keras.Sequential()

model_1.add(Dense(16, input_dim=18, activation='relu'))
model_1.add(Dropout(0.6))
model_1.add(Dense(8, activation='relu'))

# No need to modify the code under this
model_1.add(Dense(1, activation = 'linear'))
model_1.summary()

# Compile the model
model_1.compile(loss='mse', optimizer='adam', metrics=['accuracy'])


print(X_train_pp.shape)
print(y_train.shape)
history = model_1.fit(X_train_pp, y_train, epochs=100, batch_size=32, validation_split=0.2)

keras.utils.plot_model(model_1, show_shapes=True)

"""### **Second Model**"""

X_train_pp_df = DateTimeImputer().transform(X_train)
X_train_pp_df = DateDissolver(replace=True).transform(X_train_pp_df)
X_train_pp_df = text_pipeline.fit_transform(X_train_pp_df)

X_train_pp = full_pipeline.fit_transform(X_train_pp_df)

model_2 = keras.models.Sequential([
      keras.layers.Dense(72, input_shape=(18,), activation="relu"),
      keras.layers.Dropout(0.6),
      keras.layers.Dense(128, activation="relu"),
      keras.layers.Dropout(0.6),
      keras.layers.Dense(256, activation="relu"),
      keras.layers.Dropout(0.6),
      keras.layers.Dense(128, activation="relu"),
      keras.layers.Dropout(0.6),
      keras.layers.Dense(72, activation="relu"),
      keras.layers.Dropout(0.6),
      keras.layers.Dense(6)
    ])

model_2.summary()

# Compile the model
model_2.compile(loss='mse', optimizer='adam')

print(X_train_pp.shape)
print(y_train.shape)
history = model_2.fit(X_train_pp, y_train, epochs=100, batch_size=32, validation_split=0.2)

keras.utils.plot_model(model_2, show_shapes=True)

"""### **Thrid Model**"""

X_train_pp_df = DateTimeImputer().transform(X_train)
X_train_pp_df = DateDissolver(replace=True).transform(X_train_pp_df)
X_train_pp_df = text_pipeline.fit_transform(X_train_pp_df)

X_train_pp = full_pipeline.fit_transform(X_train_pp_df)

model_3 = keras.models.Sequential()
model_3.add(keras.layers.Dense(5000 , activation="relu", input_shape=X_train_pp.shape[1:]))
model_3.add(keras.layers.Dense(1000, activation="relu"))
model_3.add(keras.layers.Dense(2000, activation="relu"))
model_3.add(keras.layers.Dense(100, activation="relu"))
model_3.add(keras.layers.Dense(500, activation="relu"))
model_3.add(keras.layers.Dense(X_train_pp.shape[1], activation="relu"))
model_3.add(keras.layers.Dense(1))

model_3.compile(loss="mean_squared_logarithmic_error", optimizer=keras.optimizers.SGD(lr=1e-1))
history = model_3.fit(X_train_pp, y_train, epochs=100, batch_size=32, validation_split=0.2)

keras.utils.plot_model(model_3, show_shapes=True)

"""## **Part 4 : Prediction with the test_set**"""

X_test = test_set.copy().drop(test_set.filter(regex=COLUMN_EXCLUDE_PATTERN), axis=1)
print(X_test.info())
X_test_pp_df = DateTimeImputer().transform(X_test)
X_test_pp_df = DateDissolver(replace=True).transform(X_test_pp_df)
X_test_pp_df = text_pipeline.fit_transform(X_test_pp_df)
print(X_test_pp_df.info())

X_test_pp = full_pipeline.transform(X_test_pp_df)
X_test_pp[0]

from sklearn.metrics import mean_squared_log_error

final_model = model_3
final_predictions = final_model.predict(X_test_pp)

print("Predicts -> ", list(final_predictions[0:100]))

test_set['popularity_predicted'] = final_predictions
test_set[['id', 'popularity']].to_csv('./sample_submission.csv', header=True, index=False)

test_set[['id', 'original_title', 'popularity', 'popularity_predicted']].head(n = 50)

"""## **Part 5 : Time**

### **Time depending on the number of processors/hosts**

#### **Model training on a single worker**
"""

import json
import os
import sys

from timeit import default_timer as timer

class TimingCallback(keras.callbacks.Callback):
    def __init__(self, logs={}):
        self.logs=[]
    def on_epoch_begin(self, epoch, logs={}):
        self.starttime = timer()
    def on_epoch_end(self, epoch, logs={}):
        self.logs.append(timer()-self.starttime)

X_train_pp_df = DateTimeImputer().transform(X_train)
X_train_pp_df = DateDissolver(replace=True).transform(X_train_pp_df)
X_train_pp_df = text_pipeline.fit_transform(X_train_pp_df)

X_train_pp = full_pipeline.fit_transform(X_train_pp_df)

model_3 = keras.models.Sequential()
model_3.add(keras.layers.Dense(5000 , activation="relu", input_shape=X_train_pp.shape[1:]))
model_3.add(keras.layers.Dense(1000, activation="relu"))
model_3.add(keras.layers.Dense(2000, activation="relu"))
model_3.add(keras.layers.Dense(100, activation="relu"))
model_3.add(keras.layers.Dense(500, activation="relu"))
model_3.add(keras.layers.Dense(X_train_pp.shape[1], activation="relu"))
model_3.add(keras.layers.Dense(1))


cb = TimingCallback()

model_3.compile(loss="mean_squared_logarithmic_error", optimizer=keras.optimizers.SGD(lr=1e-1))
history = model_3.fit(X_train_pp, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[cb])

print(cb.logs)
print(sum(cb.logs))

print(history.history.keys())

# plot the training loss and accuracy
H = history.history

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""#### **Model training on a multiple worker**

https://keras.io/getting_started/faq/

https://stackoverflow.com/questions/57181551/can-i-write-a-keras-callback-that-records-and-returns-the-total-training-time/57182112

Which replicates the model on each available device and keeps the state of each model in sync
"""

X_train_pp_df = DateTimeImputer().transform(X_train)
X_train_pp_df = DateDissolver(replace=True).transform(X_train_pp_df)
X_train_pp_df = text_pipeline.fit_transform(X_train_pp_df)

X_train_pp = full_pipeline.fit_transform(X_train_pp_df)


strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
with strategy.scope():
    # This could be any kind of model -- Functional, subclass...
    model_3 = keras.models.Sequential()
    model_3.add(keras.layers.Dense(5000 , activation="relu", input_shape=X_train_pp.shape[1:]))
    model_3.add(keras.layers.Dense(1000, activation="relu"))
    model_3.add(keras.layers.Dense(2000, activation="relu"))
    model_3.add(keras.layers.Dense(100, activation="relu"))
    model_3.add(keras.layers.Dense(500, activation="relu"))
    model_3.add(keras.layers.Dense(X_train_pp.shape[1], activation="relu"))
    model_3.add(keras.layers.Dense(1))

    cb = TimingCallback()

    model_3.compile(loss="mean_squared_logarithmic_error", optimizer=keras.optimizers.SGD(lr=1e-1))

history = model_3.fit(X_train_pp, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[cb], workers=20, use_multiprocessing=True)

print(cb.logs)
print(sum(cb.logs))

print(history.history.keys())

# plot the training loss and accuracy
H = history.history

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""### **Time depending on the number of data**

We reduce the number of data. Originally, we have 3000 data. We will divise per 2 this amount and observe the accuracy of the prediction and the influence on the time.

As well, I took into consideration the thrid model
"""

reducedData = X_train.head(1500)

X_train_reducedData_pp_df = DateTimeImputer().transform(reducedData)
X_train_reducedData_pp_df = DateDissolver(replace=True).transform(X_train_reducedData_pp_df)
X_train_reducedData_pp_df = text_pipeline.fit_transform(X_train_reducedData_pp_df)

X_train_reducedData_pp = full_pipeline.fit_transform(X_train_reducedData_pp_df)

model_3_reducedData = keras.models.Sequential()
model_3_reducedData.add(keras.layers.Dense(5000 , activation="relu", input_shape=X_train_reducedData_pp.shape[1:]))
model_3_reducedData.add(keras.layers.Dense(1000, activation="relu"))
model_3_reducedData.add(keras.layers.Dense(2000, activation="relu"))
model_3_reducedData.add(keras.layers.Dense(100, activation="relu"))
model_3_reducedData.add(keras.layers.Dense(500, activation="relu"))
model_3_reducedData.add(keras.layers.Dense(X_train_reducedData_pp.shape[1], activation="relu"))
model_3_reducedData.add(keras.layers.Dense(1))


cb = TimingCallback()

model_3_reducedData.compile(loss="mean_squared_logarithmic_error", optimizer=keras.optimizers.SGD(lr=1e-1))
history = model_3_reducedData.fit(X_train_reducedData_pp, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[cb])

print(cb.logs)
print(sum(cb.logs))

"""**Prediction**"""

X_test = test_set.copy().drop(test_set.filter(regex=COLUMN_EXCLUDE_PATTERN), axis=1)
print(X_test.info())
X_train_reducedData_pp_df = DateTimeImputer().transform(X_test)
X_train_reducedData_pp_df = DateDissolver(replace=True).transform(X_train_reducedData_pp_df)
X_train_reducedData_pp_df = text_pipeline.fit_transform(X_train_reducedData_pp_df)
print(X_train_reducedData_pp_df.info())

X_test_pp = full_pipeline.transform(X_train_reducedData_pp_df)
X_test_pp[0]

from sklearn.metrics import mean_squared_log_error

final_model = model_3_reducedData
final_predictions = final_model.predict(X_test_pp)

print("Predicts -> ", list(final_predictions[0:100]))

test_set['popularity_predicted'] = final_predictions
test_set[['id', 'popularity']].to_csv('./sample_submission.csv', header=True, index=False)

test_set[['id', 'original_title', 'popularity', 'popularity_predicted']].head(n = 50)

